{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lesson, we are going to introduce the $K$-nearest neighbors (KNN) algorithm and show some practical ways of using it in `R` with the `knn` function that exists in the `class` library. Later, we will show how to tune it with the `caret` library.\n",
    "\n",
    "For a simple binary classification task (two class classification, $A$, $B$), *given* training and testing datasets and a positive integer $K$, for **each record in the test dataset**, KNN tries to find $K$ neighbors in **training set** that are *closest* to that test record and *counts* how many of those $K$ examples in the training set belong to class $A,$ and how many belong to class $B.$ The test record is then classified as belonging to the majority class (based on counted votes) i.e. the test record is considered to be of class $i$ if the majority of the $K$-nearest neighbors in the training set belong to class $i.$\n",
    "\n",
    "As can be seen, there are no parameters that *need to be learned during training* to determine whether a new observation belongs to class $A$ or $B.$ The only parameter used in k-nearest neighbors is k, which is a predetermined value. The algorithm simply works by looking at the training samples, calculating *distances* and finding the $K$ examples in the training set that are closest to the new observation. Thus, KNN is a *non-parametric,* supervised (needs training labels) learning algorithm.\n",
    "\n",
    "The following diagram illustrates the main idea of how the k-nearest neighbors algorithm works. As $K$ varies from $3$ to $6$ the class of the new observation (red star) changes from $B$ to $A$ because the majority votes are changed. That is, for $K=3,$ we have two observations of class $B$ and one of class $A$, while for $K=6,$ we have two observations of class $B,$ and four of class $A.$\n",
    "\n",
    "<img src=\"http://bdewilde.github.io/assets/images/2012-10-26-knn-concept.png\" width=600, align = \"center\">\n",
    "<div style=\"text-align:center\"> [[KNN classifications for k=3 and k=6]](http://bdewilde.github.io/blog/blogger/2012/10/26/classification-of-hand-written-digits-3/)\n",
    "<br>\n",
    "\n",
    "For a regression task, the same method can be applied but instead of taking majority votes, we can, for example, find the mean of the $response$ $variable$ of the $K$-nearest neighbors from the new observation.\n",
    "\n",
    "KNN depends on 1) the choice of metric (for example, Euclidean in above example), and 2) the choice of $K.$ There are no universal choices, and depending on the data, one has to examine various options to find a suitable choice.\n",
    "\n",
    "*Caveats:*\n",
    "\n",
    "* When using KNN, we must ensure that there are no categorical variables (factors) involved in the **features**, simply because one cannot find the distance from them. For example, when a categorical variable takes values from the set {apple, orange, banana, grapes ...}, one cannot make use of numerical distance functions, unless of course there is a pre-determined way to evaluate these distance from a qualitative standpoint.\n",
    "\n",
    "* If the training set is high-dimensional, KNN will suffer from [the curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). Therefore, we could use a dimensionality reduction technique prior to using KNN.\n",
    "\n",
    "* *Standardize* the *training* set before using KNN. Precisely, one can preprocess data so that each training feature (column) has a mean of zero and a standard deviation of one. Note that the order is exact. \n",
    "\n",
    "In fact, we will see the **effect** of standardizing training and test sets on the predicted values later.\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notice:__ We have to install some packages for this notebook, and it may takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# install the packages (note: this may take some time)\n",
    "install.packages(\"class\")\n",
    "install.packages(\"caret\")\n",
    "install.packages(\"mlbench\")\n",
    "install.packages(\"e1071\")\n",
    "\n",
    "library(class)\n",
    "library(caret)\n",
    "require(mlbench)\n",
    "library(e1071)\n",
    "library(base)\n",
    "require(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Step 1- Data collection\n",
    "\n",
    "For this lesson, we will be using `Sonar` data set (signals) from `mlbench` library. `Sonar` is a system for the detection of objects under water and for measuring the water's depth by emitting sound pulses and detecting. The complete description can be found in [mlbench](https://cran.r-project.org/web/packages/mlbench/mlbench.pdf). For our purposes, this is a two-class (class $R$ and class $M$) classification task with numeric data.\n",
    "\n",
    "Let's look at the first five rows of `Sonar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data(Sonar)\n",
    "head(Sonar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2- Preparing and exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is A data frame with 208 observations on 61 variables, all numerical and one (the Class) nominal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "cat(\"number of rows and columns are:\", nrow(Sonar), ncol(Sonar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets check how many $M$ classes and $R$ classes `Sonar` data contain? and check whether `Sonar` data contains any NA in its columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "base::table(Sonar$Class) \n",
    "apply(Sonar, 2, function(x) sum(is.na(x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we want to manually take samples from our data to split `Sonar` into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "SEED <- 123\n",
    "set.seed(SEED)\n",
    "data <- Sonar[base::sample(nrow(Sonar)), ] # shuffle data first\n",
    "bound <- floor(0.7 * nrow(data))\n",
    "df_train <- data[1:bound, ] \n",
    "df_test <- data[(bound + 1):nrow(data), ]\n",
    "cat(\"number of training and test samples are \", nrow(df_train), nrow(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's examine if the train and test samples have properly splitted with the almost the same portion of `Class` labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "cat(\"number of training classes: \\n\", base::table(df_train$Class)/nrow(df_train))\n",
    "cat(\"\\n\")\n",
    "cat(\"number of test classes: \\n\", base::table(df_test$Class)/nrow(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "To simplify our job, we can create the following data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "X_train <- subset(df_train, select=-Class)\n",
    "y_train <- df_train$Class\n",
    "X_test <- subset(df_test, select=-Class) # exclude Class for prediction\n",
    "y_test <- df_test$Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Step 3 – Training a model on data\n",
    "Now, we are going to use `knn` function from `class` library with $k=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model_knn <- knn(train=X_train,\n",
    "                 test=X_test,\n",
    "                 cl=y_train,  # class labels\n",
    "                 k=3)\n",
    "model_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Step 4 – Evaluate the model performance\n",
    "As you can see, `model_knn` with $k=3$ provides the above predictions for the test set `X_test`. Then, we can see how many classes have been correctly or incorrectly classified by comparing to the true labels as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "conf_mat <- base::table(y_test, model_knn)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "To compute the accuracy, we sum up all the correctly classified observations (located in diagonal) and divide it by the total number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "cat(\"Test accuracy: \", sum(diag(conf_mat))/sum(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "To assess whether $k=3$ is a good choice and see whether $k=3$ leads to overfitting /underfitting the data, we could use `knn.cv` which does the leave-one-out cross-validations for training set (i.e., it singles out a training sample one at a time and tries to view it as a new example and see what class label it assigns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Below are the predicted classes for the training set using the leave-one-out cross-validation. Now, let's examine its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "knn_loocv <- knn.cv(train=X_train, cl=y_train, k=3)\n",
    "knn_loocv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets create a confusion matrix to compute the accuracy of the training labels `y_train` and the cross-validated predictions `knn_loocv`, same as the above. What can you find from comparing the LOOCV accuracy and the test accuracy above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "conf_mat_cv <- base::table(y_train, knn_loocv)\n",
    "conf_mat_cv\n",
    "cat(\"LOOCV accuracy: \", sum(diag(conf_mat_cv)) / sum(conf_mat_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The difference between the cross-validated accuracy and the test accuracy shows that, $k=3$ leads to overfitting. Perhaps we should change $k$ to lessen the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Step 5 – Improve the performance of the model\n",
    "As noted earlier, we have *not* standardized (as part of preprocessing) our training and test sets. In the rest of the tutorial, we will see the effect of choosing a suitable $k$ through repeated *cross-validations* using `caret` library.\n",
    "\n",
    "In a *cross-validation* procedure: \n",
    "1. The data is divided into the finite number of mutually exclusive subsets \n",
    "2. Through each iteration, a subset is set aside, and the remaining subsets are used as the training set\n",
    "3. The subset that was set aside is used as the test set (prediction)\n",
    "\n",
    "This is a method of cross-referencing the model built using its own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "SEED <- 2016\n",
    "set.seed(SEED)\n",
    "# create the training data 70% of the overall Sonar data.\n",
    "in_train <- createDataPartition(Sonar$Class, p=0.7, list=FALSE) # create training indices\n",
    "ndf_train <- Sonar[in_train, ]\n",
    "ndf_test <- Sonar[-in_train, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we specify the cross-validation method we want to use to find the best $k$ in grid search. Later, we use the built-in `plot` function to assess the changes in accuracy for different choices of $k.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# lets create a function setup to do 5-fold cross-validation with 2 repeat.\n",
    "ctrl <- trainControl(method=\"repeatedcv\", number=5, repeats=2)\n",
    "\n",
    "nn_grid <- expand.grid(k=c(1,3,5,7))\n",
    "nn_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "set.seed(SEED)\n",
    "\n",
    "best_knn <- train(Class~., data=ndf_train,\n",
    "                  method=\"knn\",\n",
    "                  trControl=ctrl, \n",
    "                  preProcess = c(\"center\", \"scale\"),  # standardize\n",
    "                  tuneGrid=nn_grid)\n",
    "best_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "So seemingly, $k=1$ has the highest accuracy from repeated cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### References:\n",
    "\n",
    "* [K-nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "* [Short introdunction to caret](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)\n",
    "* [Predictive modeling with caret](https://www.r-project.org/nosvn/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
